# -*- coding: utf-8 -*-
"""Intermediate_asses3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yUf9KiGVIzwlmBY66NxbV1ScXD8JUno-

#Libraries
"""

import numpy as np
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler,MinMaxScaler
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier #KNN
from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,confusion_matrix,classification_report,roc_auc_score
from sklearn.naive_bayes import GaussianNB #NB
from sklearn.tree import DecisionTreeClassifier #DT
from sklearn.svm import SVC #SVM
from sklearn.model_selection import GridSearchCV #model fine tuning
from sklearn.ensemble import RandomForestClassifier  # Model Feature Selection

"""#Load Data(Training Data)"""

from google.colab import drive
drive.mount('/content/drive')

filepath='/content/drive/MyDrive/DSA ICT/Data/train_LZdllcl.csv'
trn=pd.read_csv(filepath)
trn.head()

"""#EDA"""

trn.info()

cat_cols = trn.select_dtypes(include=['object']).columns
for col in cat_cols:
  print('Coloumn : ',col)
  print(trn[col].value_counts(dropna=False))
print('Categorical columns:\n',cat_cols)

"""#Data Cleaning"""

#trn.drop(columns=['employee_id'],inplace=True)

"""Handling Duplicates"""

trn.duplicated().sum()

"""Handling Missing Values"""

trn.isnull().sum()

trn['education']=trn['education'].fillna(trn['education'].mode()[0])

trn['previous_year_rating']=trn['previous_year_rating'].fillna(trn['previous_year_rating'].mean())

trn.isnull().sum()

"""Handling Outliers"""

num_cols = trn.select_dtypes(include=['int64', 'float64']).columns
for col in num_cols:
    plt.figure()
    plt.boxplot(trn[col])
    plt.title(f'Box Plot of {col}')
    plt.ylabel(col)
    plt.show()

"""#Feature Engineering"""

trn1=trn.select_dtypes(include=['number'])
corr=trn1.corr()
corr

sns.heatmap(corr,annot=True,cmap='coolwarm')

plt.figure(figsize=(15, 10))
for i, col in enumerate(num_cols):
    plt.subplot(3, 3, i + 1) # Adjust subplot grid based on number of numerical columns
    sns.histplot(trn[col].dropna(), kde=True)
    plt.title(f'Distribution of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
plt.tight_layout()
plt.show()

"""#Encoding

Frequency Encoding
"""

region_freq_map = trn['region'].value_counts(normalize=True).to_dict()
trn['region'] = trn['region'].map(region_freq_map)

trn['region'].value_counts()

"""Label Encoding"""

label_cols = ['department', 'education', 'gender', 'recruitment_channel']

le = LabelEncoder()
for col in label_cols:
    trn[col] = le.fit_transform(trn[col])
display(trn.head())

"""#Train-Test Split"""

trn1=trn.drop(columns=['employee_id'])

X = trn1.drop('is_promoted', axis=1)
y = trn1['is_promoted']

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=55)

print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

"""#Scaling

Min-Max Scaling
"""

ss = MinMaxScaler()
X_train=ss.fit_transform(X_train)
X_test=ss.transform(X_test)

"""#ML Modeling Using Classification Methods

1.Logistic Regression
"""

lr=LogisticRegression()
lr.fit(X_train,y_train)

y_pred_lr=lr.predict(X_test)

print(f'accuracy_score= {accuracy_score(y_test,y_pred_lr)}')
print(f'precision= {precision_score(y_test,y_pred_lr)}')
print(f'f1= {f1_score(y_test,y_pred_lr)}')
print(f'recall= {recall_score(y_test,y_pred_lr)}')
print(f'conf_matrix= {confusion_matrix(y_test,y_pred_lr)}')
print(f'classification_report= {classification_report(y_test,y_pred_lr)}')

"""2.KNN

Using Euclidian
"""

#we need to figure out the optimum value for k.
#to do that, we will check which value of k is giving highest accuracy.

accuracy_list = []
neighbors_range = np.arange(1,30)

for k in neighbors_range:
  classifier = KNeighborsClassifier(n_neighbors=k , metric='minkowski',p=2)  # initialising classifier for iteration using Euclidian p=2
  #Training the ML model
  classifier.fit(X_train, y_train)
  y_pred_eu=classifier.predict(X_test)  #predict using trained ML model
  acc = accuracy_score(y_test,y_pred_eu)  # evaluate the ML model
  accuracy_list.append(acc) #appending acc list with accuracy achieved for each value of k

plt.plot(neighbors_range, accuracy_list,'*-')
plt.xlabel('K')
plt.ylabel('Accuracy')
plt.xticks(neighbors_range)
plt.grid()

"""Taking Value of k=8"""

classifier_knn = KNeighborsClassifier(n_neighbors=8, metric='euclidean')
classifier_knn.fit(X_train,y_train)
y_pred_knn_eu = classifier_knn.predict(X_test)
acc_knn=accuracy_score(y_test,y_pred_knn_eu)
prec_knn=precision_score(y_test,y_pred_knn_eu)
rec_knn=recall_score(y_test,y_pred_knn_eu)
f1_knn=f1_score(y_test,y_pred_knn_eu)

print('Accuracy:',acc_knn)
print('Precision:',prec_knn)
print('Recall:',rec_knn)
print('F1 Score:',f1_knn)
confusion_matrix(y_test,y_pred_knn_eu)

"""Using Manhattan"""

accuracy_list1 = []
neighbors_range1 = np.arange(1,30)

for k in neighbors_range:
  classifier1 = KNeighborsClassifier(n_neighbors=k , metric='minkowski',p=1)  # initialising classifier for iteration using manhattan p=1
  #Training the ML model
  classifier1.fit(X_train, y_train)
  y_pred_mn=classifier1.predict(X_test)  #predict using trained ML model
  acc1 = accuracy_score(y_test,y_pred_mn)  # evaluate the ML model
  accuracy_list1.append(acc1) #appending acc list with accuracy achieved for each value of k

plt.plot(neighbors_range1, accuracy_list1,'*-')
plt.xlabel('K')
plt.ylabel('Accuracy')
plt.xticks(neighbors_range1)
plt.grid()

classifier1_knn = KNeighborsClassifier(n_neighbors=9, metric='manhattan') #using K=9
classifier1_knn.fit(X_train,y_train)
y_pred_knn_mn = classifier1_knn.predict(X_test)
acc_knn1=accuracy_score(y_test,y_pred_knn_mn)
prec_knn1=precision_score(y_test,y_pred_knn_mn)
rec_knn1=recall_score(y_test,y_pred_knn_mn)
f1_knn1=f1_score(y_test,y_pred_knn_mn)

print('Accuracy:',acc_knn1)
print('Precision:',prec_knn1)
print('Recall:',rec_knn1)
print('F1 Score:',f1_knn1)
confusion_matrix(y_test,y_pred_knn_mn)

"""3.Naive Baye's"""

#initializing the model
nb=GaussianNB()
#training the model
nb.fit(X_train,y_train)
#predict using trained ML model
y_pred_nb=nb.predict(X_test)
#evaluate the model
acc_nb=accuracy_score(y_test,y_pred_nb)
prec_nb=precision_score(y_test,y_pred_nb)
rec_nb=recall_score(y_test,y_pred_nb)
f1_nb=f1_score(y_test,y_pred_nb)

print('Accuracy:',acc_nb)
print('Precision:',prec_nb)
print('Recall:',rec_nb)
print('F1 Score:',f1_nb)
confusion_matrix(y_test,y_pred_nb)

"""4.Decision Tree"""

#initializing the model
dt=DecisionTreeClassifier()
#training the model
dt.fit(X_train,y_train)
#predict using trained ML model
y_pred_dt=dt.predict(X_test)
#evaluate the model
acc_dt=accuracy_score(y_test,y_pred_dt)
prec_dt= precision_score(y_test,y_pred_dt)
rec_dt=recall_score(y_test,y_pred_dt)
f1_dt=f1_score(y_test,y_pred_dt)

print('Accuracy:',acc_dt)
print('Precision:',prec_dt)
print('Recall:',rec_dt)
print('F1 Score:',f1_dt)
confusion_matrix(y_test,y_pred_dt)

"""5.SVM (SVC)"""

svm_clf = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=55)  ##initializing the model
svm_clf.fit(X_train, y_train)   ##training the model
y_pred_sv = svm_clf.predict(X_test)  ##training the model

#evaluating the model
acc_sv=accuracy_score(y_test,y_pred_sv)
prec_sv= precision_score(y_test,y_pred_sv)
rec_sv=recall_score(y_test,y_pred_sv)
f1_sv=f1_score(y_test,y_pred_sv)


print('Accuracy:',acc_sv)
print('Precision:',prec_sv)
print('Recall:',rec_sv)
print('F1 Score:',f1_sv)
confusion_matrix(y_test,y_pred_sv)

"""#KNN Model Fine Tuning using Grid Search CV **(Optional)**"""

param_grid_knn = {
    'n_neighbors': list(range(1, 31)),
    'metric': ['euclidean', 'manhattan']
}

print(param_grid_knn)

knn = KNeighborsClassifier()
grid_search_knn = GridSearchCV(estimator=knn, param_grid=param_grid_knn, cv=5, scoring='accuracy', n_jobs=-1)

print("GridSearchCV for KNN initialized.")

grid_search_knn.fit(X_train, y_train)

print("GridSearchCV fitting complete.")

print("Best parameters for KNN:", grid_search_knn.best_params_)
print("Best score for KNN:", grid_search_knn.best_score_)

best_knn = grid_search_knn.best_estimator_
y_pred_best_knn = best_knn.predict(X_test)

print("KNN (Fine-Tuned) Performance on Test Set:")
print(f'Accuracy: {accuracy_score(y_test, y_pred_best_knn)}')
print(f'Precision: {precision_score(y_test, y_pred_best_knn)}')
print(f'Recall: {recall_score(y_test, y_pred_best_knn)}')
print(f'F1 Score: {f1_score(y_test, y_pred_best_knn)}')
print('Confusion Matrix:\n', confusion_matrix(y_test, y_pred_best_knn))

"""#Load Data(Testing Data)"""

from google.colab import drive
drive.mount('/content/drive')

filepath='/content/drive/MyDrive/DSA ICT/Data/test_2umaH9m.csv'
tes=pd.read_csv(filepath)
tes.head()

"""#Data Cleaning"""

tes.info()

for col in tes.columns:
    print(f"\nColumn: {col}")
    print(tes[col].value_counts(dropna=False))

"""Handling Missing Values"""

tes.isnull().sum().sort_values(ascending=False)

tes['education']=trn['education'].fillna(trn['education'].mode()[0])

tes['previous_year_rating']=tes['previous_year_rating'].fillna(tes['previous_year_rating'].mean())

tes.isnull().sum().sort_values(ascending=False)

"""#Encoding

Frequency encoding
"""

region_freq_map = tes['region'].value_counts(normalize=True).to_dict()
tes['region'] = tes['region'].map(region_freq_map)

tes['region'].value_counts()

"""Label encoding"""

for col in label_cols:
    tes[col] = le.fit_transform(tes[col])

tes.head()

"""#Scaling

Min-Max Scaling
"""

tes = tes.drop(columns=['employee_id'])

tes=ss.transform(tes)

"""#ML Modeling using fine tuned Logistic Regression"""

y_pred_tes=best_knn.predict(tes)

"""#Create submission file"""

sample_sub = pd.read_csv('sample_submission_M0L0uXE(1).csv')
sample_sub['is_promoted'] = y_pred_tes

sample_sub.to_csv('final_submission.csv', index=False)